{
  "publishedAt": "06-08-2025",
  "title": "Getting started with AI framework CrewAI",
  "content": "O﻿ne of my go-to AI frameworks at the moment is CrewAI. CrewAI offers an easily accessible framework for building conversational AI applications that is easy to setup and learn.\n\nI﻿n this article I'm describing how to setup a new application based on CrewAI and FastAPI.\n\n## P﻿rerequisites\n\nB﻿efore you get started, make sure that you have the following installed:\n\n* P﻿ython 3.10 - 3.12\n* R﻿ust\n* C﻿++\n\n## I﻿nstalling modules\n\nI﻿n this example, I'm using pip to install required modules, venv to create a local python environment and uvicorn to run the application.\n\nC﻿reate a new folder with a requirements.txt with the following contents\n\n```\ncrewai\ncrewai[tools]\ncrewai-tools[mcp]\nfastapi\nuvicorn\npython-dotenv\npydantic\ncelery\nrequests\n```\n\nN﻿ow, run the below commands to setup your local python environment and install the modules\n\n```powershell\n# Create venv\npython -m venv .venv\n\n# Activate venv\n.\\.venv\\Scripts\\activate\n\n# Install dependencies\npython -m pip install -r .\\requirements.txt\n```\n\nS﻿etting up CrewAI\n\nC﻿reate a new file called `crew.py` with the following content\n\n```python\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai_tools import (\n    ScrapeWebsiteTool\n)\nfrom pydantic import BaseModel\n\nclass SeoMetadata(BaseModel):\n    title: str\n    description: str\n    keywords: list[str]\n\nclass SeoCrew():\n    scrape_tool = ScrapeWebsiteTool()\n\n    title_agent = Agent(\n        role=\"SEO Specialist\",\n        goal=\"To write SEO metadata that are highly relevant to the content provided.\",\n        backstory=\"You are an SEO Specialist responsible for optimizing content for search engines. You have access to a variety of tools to help you with scraping website content, keyword research, content optimization, and performance tracking.\",\n        reasoning=False,\n        verbose=True\n    )\n\n    extract_task = Task(\n        description=\"Extract content from a webpage at {url}.\",\n        expected_output=\"Your output should be the main content of the webpage, excluding any advertisements or unrelated information.\",\n        agent=title_agent,\n        tools=[scrape_tool]\n    )\n\n    write_task = Task(\n        description=\"Write SEO metadata for a webpage.\",\n        expected_output=\"A JSON object with 'title' and 'description' fields and a list of 'keywords' that are relevant to the content of the webpage.\",\n        agent=title_agent,\n        context=[extract_task],\n        output_json=SeoMetadata\n    )\n\n    def crew(self) -> Crew:\n        \"\"\"Creates the crew\"\"\"\n        return Crew(\n            agents=[self.title_agent],\n            tasks=[self.extract_task, self.write_task],\n            verbose=True,\n            process=Process.sequential\n        )\n```\n\nT﻿he above code creates a Crew which includes an agent specialized in writing SEO metadata and two tasks. It has access to a task that is linked to the CrewAI Scrape Website tool, capable of retrieving content from a URL, and a second task that takes that content as input and generates relative title and description metadata values.\n\nN﻿ext, we need to include an entry point for our python application using FastAPI. Create a main.py with the following content\n\n```\nfrom fastapi import FastAPI\nfrom crew import SeoCrew\n\napp = FastAPI()\n\n@app.post(\"/run_crew/\")\nasync def run_crew_endpoint(inputs: dict = None):\n    try:\n        result = SeoCrew().crew().kickoff(inputs=inputs)\n        return {\"result\": result}\n    except ValueError as e:\n        return {\"error\": str(e)}\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n```\n\nT﻿his FastAPI application exposes a single POST endpoint at `/run_crew`, which will run our CrewAI Crew. The POST body needs to include a JSON object with the `url` variable that will be used to scrape the website.\n\nL﻿astly, you need to create a .env file to configure the LLM to be used by CrewAI. Use below variables to run your application against OpenAI's API\n\n```\nMODEL=gpt-4o-mini\nOPENAI_API_KEY=[your_openai_api_key_here]\n```\n\nYour application should be ready to run now using the following command:\n\n`uvicorn main:app --reload --port 8000`\n\nSwagger docs for your application will now be available at http://localhost:8000/docs"
}