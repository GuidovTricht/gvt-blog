{
  "publishedAt": "04-22-2025",
  "title": "OpenTelemetry - Basic Kubernetes Setup",
  "content": "This article describes how to deploy a basic setup of OpenTelemetry-supported monitoring and tracing tools on Kubernetes.\n\nOpenTelemetry offers a Kubernetes Operator and Helm charts which can be used to deploy various tools. Since Prometheus is not one of them, we have opted for using the Prometheus Operator to deploy Prometheus, and to deploy Grafana and Jaeger using regular Kubernetes Deployments.\n\nAll workloads created in this article, are created in a separate 'monitoring' namespace.\n\n## Prometheus\n\nTo deploy Prometheus, you can use the Prometheus Operator. This can be done in various ways, but in this article, we use the regular, Kubernetes manifests, method.\n\nBefore Prometheus Operator can be deployed, we need to install the Custom Resource Definitions.\\\nThe latest version can be found in their GitHub releases atÂ <https://github.com/prometheus-operator/prometheus-operator/releases/>.\n\nOnce the Custom Resource Definitions have been created, you can deploy the Prometheus Operator using the following manifest\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n  name: prometheus-operator\n  namespace: monitoring\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n  name: prometheus-operator\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/metrics\n  verbs:\n  - get\n- apiGroups:\n  - monitoring.coreos.com\n  resources:\n  - alertmanagers\n  - alertmanagers/finalizers\n  - alertmanagerconfigs\n  - prometheuses\n  - prometheuses/finalizers\n  - thanosrulers\n  - thanosrulers/finalizers\n  - servicemonitors\n  - podmonitors\n  - prometheusrules\n  - probes\n  verbs:\n  - '*'\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  verbs:\n  - '*'\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  verbs:\n  - '*'\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - list\n  - delete\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  - services/finalizers\n  - endpoints\n  verbs:\n  - get\n  - create\n  - update\n  - delete\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n  name: prometheus-operator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus-operator\nsubjects:\n- kind: ServiceAccount\n  name: prometheus-operator\n  namespace: monitoring\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --logtostderr=true\n        - --config-reloader-image=docker.io/jimmidyson/configmap-reload:v0.4.0\n        - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:v0.42.1\n        image: \"quay.io/coreos/prometheus-operator:v0.42.1\"\n        name: prometheus-operator\n        securityContext:\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 100m\n          limits:\n            memory: 200Mi\n            cpu: 200m\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: prometheus-operator\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n```\n\nThe Operator will watch for Prometheus workloads to be defined before creating Prometheus Services.\\\nBelow manifest can be used to deploy a simple Prometheus Service including an Azure Disk for its data storage.\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\n  namespace: monitoring\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/metrics\n  verbs:\n  - get\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\n  namespace: monitoring\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: prometheus\nallowVolumeExpansion: true\nprovisioner: kubernetes.io/azure-disk\nparameters:\n  storageaccounttype: Standard_LRS\n---\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus-servicemonitors\n  namespace: monitoring\nspec:\n  nodeSelector:\n    kubernetes.io/os: linux\n  securityContext:\n    runAsUser: 0\n  serviceAccountName: prometheus\n  alerting:\n    alertmanagers:\n    - namespace: monitoring\n      name: alertmanager-service\n      port: alertmanager\n  serviceMonitorSelector:\n    matchLabels:\n      type: servicemonitor\n  ruleSelector:\n    matchLabels:\n      role: alert-rules\n      prometheus: prometheus-service\n  resources:\n    requests:\n      memory: 400Mi\n      cpu: 128m\n    limits:\n      memory: 800Mi\n      cpu: 256m\n  enableAdminAPI: false\n  logLevel: warn\n  retention: 15d\n  storage:\n    volumeClaimTemplate:\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        storageClassName: prometheus\n        resources:\n          requests:\n            storage: 8Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-service\n  namespace: monitoring\nspec:\n  ports:\n  - name: http\n    port: 9090\n    protocol: TCP\n  selector:\n    prometheus: prometheus-servicemonitors\n```\n\nThe Prometheus Service will watch any ServiceMonitor definitions created and ingest data from the defined endpoints. By default, it will make requests to the `/metrics` path and ingest the data outputted there.\\\nYou will want to create a ServiceMonitor for any application you want to monitor.\n\n```yaml\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: microservice-service-monitor\n  namespace: monitoring\n  labels:\n    type: servicemonitor\nspec:\n  namespaceSelector:\n    any: true\n  selector:\n    matchLabels:\n      name: microservice\n  endpoints:\n  - targetPort: 80\n```\n\nA ServiceMonitor will expose any application it finds based on the defined selectors. In above example, a selector is used to find workloads in any namespace with the `name` label set to `microservice`.\n\n## Jaeger\n\nThe easiest way to deploy Jaeger is through a Kubernetes manifest which creates a Deployment and Service definition.\n\nBefore creating the Deployment, we need to create a ConfigMap with the necessary Jaeger configuration.\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: jaeger-collector\n  namespace: monitoring\ndata:\n  collector.yaml: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n            endpoint: 0.0.0.0:4317\n          http:\n            endpoint: 0.0.0.0:4318\n    exporters:\n      jaeger_storage_exporter:\n        trace_storage: memstore\n    extensions:\n      jaeger_query:\n        storage:\n          traces: memstore\n      jaeger_storage:\n        backends:\n          memstore:\n            memory:\n              max_traces: 100000\n    service:\n      extensions:\n        - jaeger_storage\n        - jaeger_query\n      telemetry:\n        metrics:\n          address: 0.0.0.0:8888\n      pipelines:\n        traces:\n          exporters:\n            - jaeger_storage_exporter\n          receivers:\n            - otlp\n```\n\nThis ConfigMap defines what ports Jaeger should listen on to retrieve traces.\n\nWith the ConfigMap in place, we can create the Deployment and Service definitions.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: jaeger-service\n  namespace: monitoring\nspec:\n  ports:\n  - name: jaeger\n    port: 16686\n    targetPort: jaeger\n  - name: metrics\n    port: 8888\n    targetPort: metrics\n  - name: otlp-grpc\n    port: 4317\n    targetPort: otlp-grpc\n  - name: otlp-http\n    port: 4318\n    targetPort: otlp-http\n  selector:\n    app.kubernetes.io/name: jaeger-deployment\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: jaeger-deployment\n  name: jaeger-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: jaeger-deployment\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jaeger-deployment\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: \"8888\"\n        prometheus.io/scrape: \"true\"\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      containers:\n      - args:\n        - --config=/conf/collector.yaml\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: jaegertracing/jaeger:latest\n        imagePullPolicy: Always\n        name: jaeger-deployment\n        ports:\n        - containerPort: 16686\n          name: jaeger\n          protocol: TCP\n        - containerPort: 16686\n          name: jaeger-query\n          protocol: TCP\n        - containerPort: 8888\n          name: metrics\n          protocol: TCP\n        - containerPort: 4317\n          name: otlp-grpc\n          protocol: TCP\n        - containerPort: 4318\n          name: otlp-http\n          protocol: TCP\n        resources:\n          requests:\n            memory: 100Mi\n            cpu: 100m\n          limits:\n            memory: 500Mi\n            cpu: 500m\n        volumeMounts:\n        - mountPath: /conf\n          name: otc-internal\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - configMap:\n          items:\n          - key: collector.yaml\n            path: collector.yaml\n          name: jaeger-collector\n        name: otc-internal\n```\n\nThis will deploy Jaeger in the most simplistic way, using memory for storage. Note that this does mean that the data in Jaeger will disappear and reset whenever the application is restarted.\n\nWith above configuration, OpenTelemetry traces can be sent to jaeger-service over port 4317 for GRPC or 4318 for HTTP.\n\n## Grafana\n\nGrafana can be deployed in various ways. In this example we deploy it as a simple manifest to deploy ConfigMaps, a Deployment and a Service definition.\n\nFirst, you will want to create a ConfigMap to define the data sources for Grafana:\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-datasource\n  namespace: monitoring\n  labels:\n    grafana_datasource: '1'\ndata:\n  datasource.yaml: |-\n    apiVersion: 1\n    datasources:\n    - name: Prometheus\n      uid: webstore-metrics\n      type: prometheus\n      access: proxy\n      url: http://prometheus-service.monitoring:9090/\n      isDefault: true\n    - name: Jaeger\n      uid: webstore-traces\n      type: jaeger\n      url: http://jaeger:16686/jaeger/ui\n      editable: true\n      isDefault: false\n```\n\nIn above example, two data sources will be created for Prometheus and Jaeger.\n\nAdditionally, we want to create a ConfigMap to define Grafana configuration regarding where to locate dashboards.\n\n```yaml\napiVersion: v1\ndata:\n  dashboards.yaml: |-\n    apiVersion: 1\n    providers:\n    - name: '0'\n      orgId: 1\n      folder: ''\n      folderUid: ''\n      type: file\n      disableDeletion: false\n      editable: true\n      updateIntervalSeconds: 30\n      options:\n        path: /etc/grafana/provisioning/dashboards\nkind: ConfigMap\nmetadata:\n  name: grafana-dashboards\n  namespace: monitoring\n  labels:\n    app: grafana\n```\n\nHaving created the necessary ConfigMaps, we can now deploy the Deployment and Service configurations.\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: grafana-deployment\n  name: grafana-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana-deployment\n    spec:\n      containers:\n        - image: docker.io/grafana/grafana:latest\n          name: grafana-deployment\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsNonRoot: true\n            runAsUser: 1000\n          ports:\n            - containerPort: 3000\n              name: grafana\n          resources:\n            requests:\n              memory: 100Mi\n              cpu: 100m\n            limits:\n              memory: 2500Mi\n              cpu: 500m\n          volumeMounts:\n            - mountPath: /var/lib/grafana\n              name: grafana-storage\n            - mountPath: /etc/grafana/provisioning/datasources\n              name: grafana-datasources\n              readOnly: true\n            - mountPath: /etc/grafana/provisioning/dashboards\n              name: grafana-dashboards\n              readOnly: true\n            - mountPath: /etc/grafana/provisioning/dashboards/rabbitmq-overview-dashboard\n              name: rabbitmq-overview-dashboard\n              readOnly: true\n            - mountPath: /etc/grafana/provisioning/dashboards/dotnet-overview-dashboard\n              name: dotnet-overview-dashboard\n              readOnly: true\n            - mountPath: /etc/grafana/provisioning/dashboards/redis-overview-dashboard\n              name: redis-overview-dashboard\n              readOnly: true\n          livenessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n              httpHeaders:\n                - name: X-Kubernetes-Probe\n                  value: Liveness\n            timeoutSeconds: 300\n            periodSeconds: 30\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n              httpHeaders:\n                - name: X-Kubernetes-Probe\n                  value: Liveness\n            timeoutSeconds: 300\n            periodSeconds: 30\n            failureThreshold: 3\n      volumes:\n        - name: grafana-storage\n          emptyDir: {}\n        - name: grafana-datasources\n          configMap:\n            name: grafana-datasource\n        - configMap:\n            name: grafana-dashboards\n          name: grafana-dashboards\n        - name: rabbitmq-overview-dashboard\n          configMap:\n            name: rabbitmq-overview-dashboard\n        - name: dotnet-overview-dashboard\n          configMap:\n            name: dotnet-overview-dashboard\n        - name: redis-overview-dashboard\n          configMap:\n            name: redis-overview-dashboard\n      nodeSelector:\n        kubernetes.io/os: linux\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: grafana-service\n  namespace: monitoring\nspec:\n  ports:\n    - port: 3000\n      protocol: TCP\n  selector:\n    app.kubernetes.io/name: grafana-deployment\n```\n\nYou may have noticed that, in above example, we specified three volumes and volumeMounts for dashboards.\\\nThese dashboards have also been created as ConfigMaps, where the JSON schema of a Grafana dashboard is stored in the below way.\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: redis-overview-dashboard\n    namespace: monitoring\n    labels:\n        app: grafana\ndata:\n    kubernetes.json: |-\n        {\n            ... JSON here\n        }\n```",
  "seoDescription": "This article describes how to deploy a basic setup of OpenTelemetry-supported monitoring and tracing tools, Prometheus, Jaeger and Grafana, on Kubernetes."
}